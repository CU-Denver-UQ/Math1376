{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math  1376: Programming for Data Science\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #We will use numpy in this lecture\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.patches import Polygon \n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "from mpl_toolkits.mplot3d import axes3d #This enables 3d plotting\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 6: The beginnings of Machine Learning\n",
    "\n",
    "In this lecture, we begin exploring how to use Python to perform the basics of machine learning. \n",
    "We are primarily going to touch on material covered in the initial chapters out of Sebastian Raschka's book [Python Machine Learning (3rd edition)](https://github.com/rasbt/python-machine-learning-book-3rd-edition). \n",
    "Specifically, we will selectively sample material from the following chapters: \n",
    "\n",
    "- Chapter 2: Training Machine Learning Algorithms for Classification\n",
    "- Chapter 3: A Tour of Machine Learning Classifiers Using Scikit-Learn\n",
    "- Chapter 4: Building Good Training Datasets â€“ Data Preprocessing\n",
    "- Chapter 5: Compressing Data via Dimensionality Reduction\n",
    "- Chapter 6: Learning Best Practices for Model Evaluation and Hyperparameter Tuning\n",
    "\n",
    "In the spirit of making arbitrary and capricious decisions, I declare that everything beyond these chapters is *advanced* and left for topics in a future course or self-study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a): Training Machine Learning Algorithms for Classification (and a gentle introduction to the Python `class`)\n",
    "\n",
    "---\n",
    "\n",
    "### Some useful source material \n",
    "\n",
    "- https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch02/ch02.ipynb\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Perceptron\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Artificial_neuron\n",
    "\n",
    "- https://en.wikipedia.org/wiki/ADALINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first classifier: the perceptron (Approx. 1 hour and 15 minutes)\n",
    "---\n",
    "\n",
    "The perceptron was developed more than half a century ago based on a neuron model. \n",
    "When you hear of neural networks or deep learning, you are *basically hearing* about something that connects many things that are *like* perceptrons (there are other types of neuron models) together. \n",
    "So, perceptrons are a good place to start our (shallow) foray into machine leaning.\n",
    "\n",
    "From Wikipedia (emphasis my own):\n",
    "\n",
    "> In machine learning, the perceptron is an algorithm for **supervised learning** of **binary classifiers**. A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.\n",
    "\n",
    "What does **supervised learning** mean? Simply put, it means we learn a function that maps inputs to outputs based on *training data* containing example input-output pairs. \n",
    "\n",
    "Mathematically, suppose that $\\mathbf{x}\\in\\mathbb{R}^m$ denotes the $m$ features present in the data, then a linear binary classifier defined on $\\mathbf{x}$ is represented as the function \n",
    "$$\n",
    "\\large    f(\\mathbf{w}\\cdot\\mathbf{x} + b) = \\begin{cases}\n",
    "                        1, & \\mathbf{w}\\cdot\\mathbf{x} + b>0, \\\\\n",
    "                        -1, & \\text{else}.\n",
    "                    \\end{cases}\n",
    "$$\n",
    "Here, $\\mathbf{w}$ denotes a $m$-dimensional vector of weights and $b$ is the bias. Both of these must be *learned* from the training data. \n",
    "In the evaluation of $f(\\mathbf{w}\\cdot\\mathbf{x} + b)$, we must compute $\\mathbf{w}\\cdot\\mathbf{x}=\\sum_{i=1}^m w_ix_i$, which denotes the standard dot product, and can be computed using the `numpy.dot` function that takes as arguments two arrays.\n",
    "We use the perceptron to make binary decisions, i.e., whether or not data that possess $m$ identifying features should belong to one set or its complement (which are represented by the outputs 1 and -1, respectively). \n",
    "\n",
    "<img src=\"images/neuron_and_perceptron.png\" width=60%>\n",
    "\n",
    "### A note about various source materials\n",
    "---\n",
    "\n",
    "As shown in the figure above (adapted from some of the [source material](https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch02/ch02.ipynb)), it is quite typical to refer to the bias as the weight $w_0$ and consider the weight vector as $(m+1)$-dimensional instead of $m$-dimensional. Keep this in mind when reviewing other sources. We will *not* follow that convention here because the formulas involving the learning of the bias are *different* than the formulas involved with learning the weights $w_1,w_2,\\ldots, w_m$. I do not want to conflate the two.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An analogy\n",
    "---\n",
    "As an analogy to statistics, recall the standard regression problem of finding a line of best fit given data $\\{(x_i,y_i)\\}_{i=1}^N$ where the input data $x$ is 1-dimensional and the output data $y$ is also 1-dimensional.\n",
    "In that problem, we *assume* (usually after inspection of a scatter plot of the data) that a line is a good description of the trend of the data so that our best prediction of $y$ given some value of $x$ is given by $y=f(x)=ax+b$. \n",
    "Then, the goal is to *learn* the values of $a$ (which is analogous to the weight vector) and $b$ (which describes a type of bias) from the *training data* $\\{(x_i,y_i)\\}_{i=1}^N$.\n",
    "There are many ways to solve the regression problem (e.g., using least squares techniques to determine $a$ and $b$), to analyze the *goodness of fit* of the line, and to study whether any of the underlying assumptions in the formulation/solution of the problem are violated. \n",
    "\n",
    "The difference for the perceptron is that the output $y=f(\\mathbf{w}\\cdot\\mathbf{x} + b)$ is either 1 or -1 (not a continuous variable), so we must consider some different algorithmic approaches for solving this problem (i.e., for learning the weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When should this work?\n",
    "---\n",
    "\n",
    "We need the data set to be ***linearly separable***. The idea that linearity plays some role is perhaps not surprising given that the analogy we used involved a line of best fit.\n",
    "\n",
    "What does linearly separable mean? It means that we can construct a line (for 2-dimensional input data sets), a plane (for 3-dimensional input data sets), or hyperplanes (for 4- and higher-dimensional input data sets) that *separates* the input data space into two parts associated with the output classes (i.e., the values of $y$). \n",
    "\n",
    "<img src=\"images/linearly_separable_example.png\" title='Linearly separable example' width=30%>\n",
    "\n",
    "> In practice, we often settle for ***almost*** linear separability, which will lead to classification errors. But, hey, nothing in life is perfect.\n",
    "\n",
    "<img src=\"images/almost_linearly_separable_example.png\" title='Almost separable' width=30%>\n",
    "\n",
    "> Hey, I have heard about this thing called a ***kernel trick***. What does that mean? While we will get to this in a later lecture, the basic premise is this: Maybe there is a function (called the kernel) that pre-processes the data in such a way that this pre-processed data is linearly separable. Then, we simply carry out our usual analysis except on this transformed data. Sounds great, right? When can we do this? Well, if it looks like there is a way to separate the data in some nonlinear way, then there is hope that we can figure out some kernel that allows us to linearly separate the transformed data. \n",
    "\n",
    "<img src=\"images/not_linearly_separable_example.png\" title='Nonlinearly separable illustration (what is the kernel trick?)' width=30%>\n",
    "\n",
    "See the following for some more background and illustrative examples:\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Linear_separability\n",
    "\n",
    "- https://www.commonlounge.com/discussion/6caf49570d9c4d0789afbc544b32cdbf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful interpretations, terminology, details, and an important lesson\n",
    "---\n",
    "\n",
    "We first consider a decidedly *bad* example based on my creating this lecture mere days after the NFL \"Zoom\" Draft of 2020. \n",
    "\n",
    "At the NFL combine, players that may be drafted have various physical and mental attributes measured. \n",
    "For player $X$, let $\\mathbf{x}=(x_1,x_2,\\ldots,x_m)$ denote the $m$-attributes that are measured. \n",
    "Perhaps $x_1$ denotes height, $x_2$ denotes the weight, $x_3$ denotes the 40-yard dash time, $x_4$ denotes the number of times the player bench presses 225lbs, $x_5$ denotes how high the player can jump, and so on (maybe the last attribute $x_m$ denotes their score on the Wonderlic test, which seems to be important more for quarterbacks and offensive lineman although even that is debatable). \n",
    "\n",
    "\n",
    "- The vector $\\mathbf{x}$ is often referred to as the *feature vector* with the $i$th component, denoted by $x_i$, being the $i$th feature of the data.\n",
    "\n",
    "\n",
    "A practical question is: can we analyze past combine data for each position and develop a perceptron to predict whether or not a player at a particular position will perform at a pro-bowl level?\n",
    "\n",
    "\n",
    "- The outputs $y$ are the classes and may be (and in fact often are) *categorical* in nature (e.g., whether or not a player is a pro-bowl level talent). \n",
    "\n",
    "\n",
    "- The weight vector $\\mathbf{w}$ and bias $b$ determines how to combine these features into a *net input* defined by $\\mathbf{w}\\cdot\\mathbf{x}+b$ that is a scalar value quantifying the total *net* features of a sample $\\mathbf{x}$. \n",
    "\n",
    "\n",
    "- The function $f$ is sometimes called the *activiation* function (especially in the context of neural networks). While we write $f(\\mathbf{w}\\cdot\\mathbf{x}+b)$, we should probably write \n",
    "<br>\n",
    "\n",
    "    - The values of 1 and 0 for $f$ are arbitrary. These are sometimes taken to be 1 and -1, but in reality, you can code these up to be any two different values you would like (but 1 and 0 or 1 and -1 are the standard choices). The whole point is just to give an output that tells you whether or not a given feature vector $\\mathbf{x}$ associated with some sample belongs to a desired class or not (e.g., whether or not some player that may be drafted at a particular position will be a pro-bowl level talent). Thus, whatever the output values are for $f$, one must then *map* the categorical values of $y$ to these values (e.g., maybe map pro-bowler to 1 and not a pro-bowler to 0). \n",
    "    \n",
    "### Wait a minute, what happens if there are multiple classes?\n",
    "\n",
    "There are plenty of prestigious accolades an NFL player may receive: Pro-Bowler, All-Pro, All-Decade, Hall of Fame, or even all of these. \n",
    "So, what happens in this case? \n",
    "Or, in the language of perceptons, how do we deal with multiple output classes, i.e., when $y$ can taken on more than two values so that the function $f$ we seek is no longer binary?\n",
    "\n",
    "To keep this lecture focused on the basics, we punt on that for the moment (sorry, couldn't resist).\n",
    "We do return to this question in your assignment. \n",
    "\n",
    "\n",
    "### An important lesson\n",
    "Why did I say this is a bad example? Well, I pulled *years* (5-10 years) of combine data from pro-football-reference.com at various skill positions (QB, RB, WR), and I could *not* see any approximate separability (even with a kernel trick) in the data. I *think* there are too many confounding factors in the data that impact player success beyond individual physical/mental features such as the quality of the team that drafts the player, how the player's talents fit into the style of the team, and of course injuries (and suspensions) that derail many careers. \n",
    "\n",
    "***So, what is the lesson?*** It is basically the same lesson you probably learned when computing lines of best fit, which is that not every data set should be modeled with a line! Data science follows the same rules as all of computational science and statistics in that no single method should be universally applied to every problem. This is just a silly thing to assume. Best to get this point deep into your mind as early as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A realistic example from realistic data\n",
    "\n",
    "John Hollinger is a former ESPN analyst/writer who developed some advanced statistics to quantify efficiencies of players and teams in the NBA. He parlayed this into becoming the VP of basketball operations for the Memphis Grizzlies in 2012. If you have ever heard of PER (Player Efficiency Rating) when evaluating a player's chances of winning awards such as MVP, then you have at least implicitly heard of Mr. Hollinger. \n",
    "\n",
    "Below, we *crawl* through some online data courtesy of ESPN. By courtesy of ESPN, I mean that according to the terms of service from Disney (the parent company of ESPN): \n",
    "\n",
    "> G. Informational and Entertainment Purposes. You understand that the Disney Products are for your personal, noncommercial use and are intended for informational and entertainment purposes only; the content available does not constitute legal, financial, professional, medical or healthcare advice or diagnosis and cannot be used for such purposes.\n",
    "\n",
    "Anyway, let's see how well team efficiency statistics in the regular season correspond to a predictor of *success* defined by a team making the playoffs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first orient ourselves on the types of data sets we will be using\n",
    "df_regular_season_temp = pd.read_html('http://www.espn.com/nba/hollinger/teamstats/_/year/2019')[0]\n",
    "\n",
    "df_postseason_temp = pd.read_html('http://www.espn.com/nba/hollinger/teamstats/_/year/2019/seasontype/3')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regular_season_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postseason_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Off_Eff = []\n",
    "Def_Eff = []\n",
    "\n",
    "y = []\n",
    "\n",
    "iter = 0\n",
    "for year in range(2009, 2020):\n",
    "    url = 'http://www.espn.com/nba/hollinger/teamstats/_/year/' + str(year)\n",
    "    df_regular_season = pd.read_html(url)[0]\n",
    "    Off_Eff.append(df_regular_season.loc[2:, 10].values)\n",
    "    Def_Eff.append(df_regular_season.loc[2:, 11].values)\n",
    "    \n",
    "    N = len(df_regular_season.loc[2:, 1].values)\n",
    "    \n",
    "    y.append(-np.ones(N))\n",
    "    url += '/seasontype/3'\n",
    "    df_playoffs = pd.read_html(url)[0]\n",
    "    \n",
    "    playoff_teams = df_playoffs.loc[2:,1].values\n",
    "    all_teams = df_regular_season.loc[2:,1].values\n",
    "    for i in range(16):\n",
    "        for j in range(N):\n",
    "            if playoff_teams[i] == all_teams[j]:\n",
    "                y[iter][j] = 1\n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Off_Eff_All = np.concatenate(Off_Eff).astype('float')\n",
    "Def_Eff_All = np.concatenate(Def_Eff).astype('float')\n",
    "\n",
    "bball_features = np.vstack((Off_Eff_All.flatten(), Def_Eff_All.flatten())).T\n",
    "\n",
    "playoffs_All = np.concatenate(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_playoffs = np.where(playoffs_All==1)[0]\n",
    "idx_no_playoffs = np.where(playoffs_All==-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.scatter(bball_features[idx_playoffs, 0], bball_features[idx_playoffs, 1], \n",
    "            s=20, c='r', marker='s')\n",
    "\n",
    "plt.scatter(bball_features[idx_no_playoffs,0], bball_features[idx_no_playoffs, 1], \n",
    "            s=20, c='b', marker='o')\n",
    "\n",
    "plt.xlabel('OFF EFF', fontsize=18)\n",
    "plt.ylabel('DEF EFF', fontsize=18)\n",
    "\n",
    "plt.title('Playoff Teams (red) and Non-Playoff Teams (blue)', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay, now what?\n",
    "\n",
    "The data above sure *looks* like it is reasonably separated into two distinct classes although there is a *bit* of overlap. It is *not* linearly separable, which will mean bad things for the convergence of the training algorithm for the perceptron. We can still *try* to train a perceptron on this data. It just may not work. (Okay, it is *guaranteed* to not work.)\n",
    "\n",
    "However, we still need to address one key point: how do we *train* the perceptron?! In other words, how do we learn the weights and bias required to define the line that separates the *feature space* of offensive and defensive efficiency into playoff vs non-playoff teams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A training algorithm for learning weights and bias\n",
    "---\n",
    "\n",
    "First some notation and terminology. \n",
    "\n",
    "- Let $\\{(\\mathbf{x}^{(i)},y^{(i)})\\}_{i=1}^N=\\{(x_1^{(i)},x_2^{(i)},\\ldots,x_m^{(i)},y^{(i)})\\}_{i=1}^N$ denote the $N$ training data we use to learn the weights $\\mathbf{w}\\in\\mathbb{R}^m$ and bias $b$. For simplicity, we assume that $y$ values are already scalarized to be 1's and -1's depending on the output classification.\n",
    "\n",
    "\n",
    "- We conceptualize the *optimal* weights and bias as the vector $\\mathbf{w}^\\text{opt}$ and scalar $b^\\text{opt}$ such that $y^{(i)}=f(\\mathbf{w}^\\text{opt}\\cdot\\mathbf{x}^{(i)}+b^\\text{opt})$ for all $1\\leq i\\leq N$.\n",
    "\n",
    ">  When data are only *almost* linearly separable instead of absolutely linearly separable, we are going to have to live with some misclassifications (i.e., *predictions* given by $f(\\mathbf{w}^\\text{opt}\\cdot\\mathbf{x}^{(i)}+b^\\text{opt})$  that do not equal the reality of $y^{(i)}$ for some subset of $1\\leq i\\leq N$). Subsequently, we expect that what is *optimal* will in general mean that we minimize the misclassification percent rather than simply make it zero. This idea is explored more in our next neuron model ADALINE, so just be patient for now.\n",
    "\n",
    "Since what is optimal is determined by solutions to $$y^{(i)}=f(\\mathbf{w}^\\text{opt}\\cdot\\mathbf{x}^{(i)}+b^\\text{opt}) $$ which implies $$y^{(i)}-f(\\mathbf{w}^\\text{opt}\\cdot\\mathbf{x}^{(i)}+b^\\text{opt})=0,$$ we suspect that as with most root-finding algorithms, we need an initial guess of the weights and bias. \n",
    "This gives us the first step of the training algorithm.\n",
    "\n",
    "**Step 1 (Initialize):** Initialize the weight vector $\\mathbf{w}^{(0)}$ and bias $b^{(0)}$ (zero or small random numbers are standard options).\n",
    "\n",
    "The goal is to determine some systematic way to incrementally update the weights and bias so that they approach their optimal values. But, how?\n",
    "\n",
    "For simplicity in developing the idea, assume $m=1$ (i.e., there is only one feature to the data so $\\mathbf{x}=x$ and the $i$th input sample is denoted by $x^{(i)}$), then we seek a weight $w^\\text{opt}$ and bias $b^\\text{opt}$ such that $w^\\text{opt}x^{(i)}+b^\\text{opt}>0$ whenever $y^{(i)}=1$ and is less than or equal to zero whenever $y^{(i)}=-1$.\n",
    "\n",
    "Having updated the weight and bias based on the first $i-1$ data points, suppose we evaluate $f(x^{(i)})$ with these values and observe one of the following scenarios:\n",
    "\n",
    "<img src=\"images/misclassification.png\" title=\"Misclassification implies we need to update weights and bias\" width=50%>\n",
    "\n",
    "We need to update the weights and bias to shift the function $f$ either to the left or right to try to remove this misclassification. \n",
    "Increasing the net input $wx+b$ will shift the function to the left while decreasing the net input $wx+b$ will shift the function to the right. \n",
    "If this is confusing or seems backwards, try plotting $f(x)=x$ and then plot $f(x+1)=x+1$ and $f(x-1)=x-1$. \n",
    "\n",
    "Well, $y^{(i)}-f(w^{(i-1)}x^{(i)}+b^{(i)})=2$ if we need to shift the function to the left and is $-2$ if we need to shift the function to the right. This suggests we can use this misclassification to determine the *sign* of the updates to the weights and bias. \n",
    "\n",
    "The updating is in fact given as follows\n",
    "\n",
    "**Step 2 (Updating):** For $i=1,2,\\ldots, N$, do the following:\n",
    " \n",
    " - Let $\\mathbf{w}^{(i-1)}$ and $b^{(i-1)}$ denote the weights and bias used to predict the $i$th output data $y^{(i)}$. \n",
    " \n",
    " - Let $0<r\\leq 1$ denote a ***learning rate***. \n",
    " \n",
    " - Compute misclassification error: $$y^{(i)}-f(w^{(i-1)}x^{(i)}+b^{(i)})$$\n",
    " \n",
    "     - Here, $f(x^{(i)})$ uses $\\mathbf{w}^{(i-1)}$ and $b^{(i-1)}$ \n",
    "     \n",
    "     - The misclassification error is 0 if there is no misclassification and is either 2 or -2 if there is misclassification error.\n",
    "\n",
    " - Update the weights $$\\mathbf{w}^{(i)} = \\mathbf{w}^{(i-1)} + r\\left[y^{(i)}-f(w^{(i-1)}x^{(i)}+b^{(i)})\\right]\\mathbf{x}^{(i)}$$\n",
    " and the bias $$b^{(i)} = b^{(i-1)} + r\\left[y^{(i)}-f(w^{(i-1)}x^{(i)}+b^{(i)})\\right].$$\n",
    " \n",
    "**Step 3 (Rinse and Repeat):** We repeat Step 2 until either the misclassifications disappear (or fall below some tolerance) or a maximum number of iterations are reached. The number of times we loop through all the training data to update the weights are called the ***epochs*** of the algorithm. If all misclassification errors disappear, then call the final weights and bias \"optimal.\" If there are still some misclassification errors, then call these final weights and bias \"near optimal.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epochs, learning rate, say what now?\n",
    "---\n",
    "\n",
    "The learning rate and epochs discussed in steps 2 and 3 above are called ***hyperparameters***. A hyperparameter refers to some variable whose value requires specification in order to run the algorithm. \n",
    "\n",
    "What values should we choose for hyperparameters? The answer is: *it depends*. \n",
    "\n",
    "Depends on what? The answer is: *on the problem*. \n",
    "\n",
    "Oh, well that just clears up everything doesn't it? Problem dependent choices for hyperparameters are necessary to get optimal performance of the algorithm. How is that helpful? The answer is: *it's not*.\n",
    "\n",
    "You are welcome! A not helpful answer is just what you wanted, isn't it? Well, we will actually see in some upcoming lectures how we go about trying to choose values of hyperparameters that are *nearly* optimal based on the data we have to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the performance of the algorithm over epochs\n",
    "---\n",
    "\n",
    "We first visualize the performance with computations below using 1- and 2-dimensional feature spaces involving ***linearly separable*** data, i.e., data for which the algorithm *should* converge if the *epochs* are either sufficiently high (relative to the size of the learning rate and quality of the initial guess).\n",
    "\n",
    "We will first interrogate the performance on linearly separable data sets that have nice \"fat\" gaps between the two data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_1d(x,w,b):\n",
    "    n = np.size(x)\n",
    "    z = -np.ones(n)\n",
    "    idx = np.where(w*x+b>0)[0]\n",
    "    z[idx] = 1\n",
    "    return z\n",
    "\n",
    "def visualize_updates_1d_perceptron(x, y, w, b, r, num_epochs):\n",
    "    \n",
    "    x_plot = np.linspace(np.min(x)-0.1*(np.max(x)-np.min(x)), \n",
    "                         np.max(x)+0.1*(np.max(x)-np.min(x)), \n",
    "                         101)\n",
    "    y_plot = f_1d(x_plot, w, b)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    ax.plot(x_plot, y_plot, 'b', linewidth=2, label='initial perceptron')\n",
    "\n",
    "    ax.set_ylim([-1.25, 1.75])\n",
    "    ax.scatter(x, y, s=50, c='r')\n",
    "\n",
    "    num_training_data = np.size(x)\n",
    "    \n",
    "    ##########################################\n",
    "    # The updates!\n",
    "    ##########################################\n",
    "    for j in range(num_epochs):\n",
    "        \n",
    "        for i in range(num_training_data):\n",
    "            \n",
    "            error = y[i] - f_1d(x[i], w, b)\n",
    "            w += r*error*x[i]\n",
    "            b += r*error\n",
    "    ##########################################\n",
    "    # End of updates\n",
    "    ##########################################\n",
    "    \n",
    "    y_plot = f_1d(x_plot, w, b)\n",
    "    ax.plot(x_plot, y_plot, 'k:', linewidth=2, label='learned perceptron')\n",
    "    ax.legend(fontsize=20)\n",
    "    ax.text(np.mean(x_plot), 1.1, r\"Learned $w$=%3.2f and $b$=%3.2f\" %(w,b),\n",
    "            horizontalalignment='center', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out\n",
    "\n",
    "# Define function to linearly separate data on real line\n",
    "def separate_line(x, pt2):\n",
    "    # the interval [pt1, pt2] separates class -1 from class 1\n",
    "    n = np.size(x)\n",
    "    y = -np.ones(n)\n",
    "    idx = np.where(x>pt2)\n",
    "    y[idx] = 1\n",
    "    return y\n",
    "\n",
    "# Create training data\n",
    "num_training = 100\n",
    "x_min = -20\n",
    "x_max = 50\n",
    "pt1 = 10 # where to separate into classes\n",
    "pt2 = 15 # remember to make pt2 > pt1\n",
    "# Probability of a training point ending up in [x_min, pt1] vs [x_max, pt2] given by proportion of lengths\n",
    "prob1 = (pt1-x_min)/((pt1-x_min)+(x_max-pt2))\n",
    "prob2 = (x_max-pt2)/((pt1-x_min)+(x_max-pt2))\n",
    "pseudo_data = np.random.uniform(low=0, high=1, size=num_training)\n",
    "x_data = np.zeros(num_training)\n",
    "for i in range(num_training):\n",
    "    if pseudo_data[i]<prob1:\n",
    "        x_data[i] = np.random.uniform(low=x_min, high=pt1)\n",
    "    else:\n",
    "        x_data[i] = np.random.uniform(low=pt2, high=x_max)\n",
    "y_data = separate_line(x_data, pt2)\n",
    "\n",
    "interact(visualize_updates_1d_perceptron, \n",
    "         x = widgets.fixed(x_data),\n",
    "         y = widgets.fixed(y_data),\n",
    "         w = widgets.FloatSlider(value=-5, min=-10, max=10, step=0.1),\n",
    "         b = widgets.FloatSlider(value=1, min=-10, max=10, step=0.1), \n",
    "         r = widgets.FloatSlider(value=0.5, min=0.01, max=1, step=0.01), \n",
    "         num_epochs = widgets.IntSlider(value=1, min=0, max=1000, step=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_2d(x, w, b):\n",
    "    n = x.shape[0]\n",
    "    z = -np.ones(n)\n",
    "    idx = np.where(np.dot(x,w)+b>0)[0]\n",
    "    z[idx] = 1\n",
    "    return z\n",
    "\n",
    "def visualize_updates_2d_perceptron(x, y, w1, w2, b, r, num_epochs):\n",
    "    \n",
    "    w = np.array([w1, w2])\n",
    "    \n",
    "    x1_plot = np.linspace(np.min(x[:,0])-0.1*(np.max(x[:,0])-np.min(x[:,0])), \n",
    "                          np.max(x[:,0])+0.1*(np.max(x[:,0])-np.min(x[:,0])), \n",
    "                          51)\n",
    "    x2_plot = np.linspace(np.min(x[:,1])-0.1*(np.max(x[:,1])-np.min(x[:,1])), \n",
    "                          np.max(x[:,1])+0.1*(np.max(x[:,1])-np.min(x[:,1])), \n",
    "                          51)\n",
    "    x1_plot, x2_plot = np.meshgrid(x1_plot, x2_plot)\n",
    "    x_plot = np.vstack((x1_plot.flatten(), x2_plot.flatten())).T\n",
    "    y_plot = f_2d(x_plot, w, b)\n",
    "    y_plot = y_plot.reshape(x1_plot.shape)\n",
    "\n",
    "    fig, axs = plt.subplots(1,2,figsize=(20,10))\n",
    "    \n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    axs[0].contourf(x1_plot, x2_plot, y_plot , alpha=0.2, cmap=cmap)\n",
    "    \n",
    "    idx_1 = np.where(y==1)[0]\n",
    "    idx_0 = np.where(y==-1)[0]\n",
    "    \n",
    "    axs[0].scatter(x[idx_1,0], x[idx_1,1], s=50, c='r', marker='x')\n",
    "    axs[0].scatter(x[idx_0,0], x[idx_0,1], s=50, c='b', marker='o')\n",
    "    axs[0].set_title('Initial perceptron', fontsize=18)\n",
    "    \n",
    "    num_training_data = x.shape[0]\n",
    "    \n",
    "    for j in range(num_epochs):\n",
    "        \n",
    "        for i in range(num_training_data):\n",
    "            \n",
    "            error = y[i] - f_2d(x[[i],:], w, b)\n",
    "            w += r*error*x[i,:]\n",
    "            b += r*error\n",
    "    \n",
    "    y_plot = f_2d(x_plot, w, b)\n",
    "    y_plot = y_plot.reshape(x1_plot.shape)\n",
    "    \n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    axs[1].contourf(x1_plot, x2_plot, y_plot , alpha=0.2, cmap=cmap)\n",
    "    \n",
    "    axs[1].scatter(x[idx_1,0], x[idx_1,1], s=50, c='r', marker='x')\n",
    "    axs[1].scatter(x[idx_0,0], x[idx_0,1], s=50, c='b', marker='o')\n",
    "    \n",
    "    axs[1].set_title('Learned $w$=(%3.2f,%3.2f) and $b$=%3.2f' %(w[0], w[1], b), fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out\n",
    "\n",
    "# Create training data\n",
    "num_training = 100\n",
    "bbox_1 = np.array([[0.5, 0], [1.5, 4]]) #first row is lower-left point and second row is upper-right point of class -1\n",
    "bbox_2 = np.array([[2, 2], [4, 4]]) # \" \" of class 1\n",
    "# Will assume equal probability of training data being in either box\n",
    "pseudo_data = np.random.uniform(low=0, high=1, size=num_training)\n",
    "x_data = np.zeros((num_training,2))\n",
    "y_data = -np.ones(num_training)\n",
    "for i in range(num_training):\n",
    "    if pseudo_data[i]<0.5:\n",
    "        x_data[i,0] = np.random.uniform(low=bbox_1[0,0], high=bbox_1[1,0])\n",
    "        x_data[i,1] = np.random.uniform(low=bbox_1[0,1], high=bbox_1[1,1])\n",
    "    else:\n",
    "        x_data[i,0] = np.random.uniform(low=bbox_2[0,0], high=bbox_2[1,0])\n",
    "        x_data[i,1] = np.random.uniform(low=bbox_2[0,1], high=bbox_2[1,1])\n",
    "        y_data[i] = 1\n",
    "\n",
    "interact(visualize_updates_2d_perceptron, \n",
    "         x = widgets.fixed(x_data),\n",
    "         y = widgets.fixed(y_data),\n",
    "         w1 = widgets.FloatSlider(value=1, min=-10, max=10, step=0.1),\n",
    "         w2 = widgets.FloatSlider(value=-1, min=-10, max=10, step=0.1),\n",
    "         b = widgets.FloatSlider(value=2, min=-10, max=10, step=0.1), \n",
    "         r = widgets.FloatSlider(value=0.1, min=0.01, max=1, step=0.01), \n",
    "         num_epochs = widgets.IntSlider(value=1, min=0, max=1000, step=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens on the basketball data that is *almost* linearly separable (meaning, not linearly separable)\n",
    "\n",
    "- Well, convergence is *not* going to happen, but with enough epochs, the updates may just *settle down* towards some good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%reset -f out\n",
    "\n",
    "interact(visualize_updates_2d_perceptron, \n",
    "         x = widgets.fixed(bball_features),\n",
    "         y = widgets.fixed(playoffs_All),\n",
    "         w1 = widgets.FloatSlider(value=1, min=-10, max=10, step=0.1),\n",
    "         w2 = widgets.FloatSlider(value=-1, min=-10, max=10, step=0.1),\n",
    "         b = widgets.FloatSlider(value=-2, min=-10, max=10, step=0.1), \n",
    "         r = widgets.FloatSlider(value=0.1, min=0.01, max=1, step=0.01), \n",
    "         num_epochs = widgets.IntSlider(value=1, min=0, max=1000, step=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADALINE = ADAptive LInear NEuron (https://en.wikipedia.org/wiki/ADALINE) (Approx time: 1 hour and 15 minutes)\n",
    "---\n",
    "\n",
    "- ADALINE is an alternative neuron model developed a few years after the perceptron was introduced. \n",
    "<br>\n",
    "\n",
    "- Unlike the perceptron that learns the weights and bias based on the misclassification error being -2, 0, or 2 due to the use of a step-activation function acting on the *net input function* defined by $\\mathbf{w}\\cdot\\mathbf{x} + b$, the activation function in ADALINE is the identity function, which means we simply use the errors defined by $y^{(i)} - \\left(\\mathbf{w}\\cdot\\mathbf{x}^{(i)} + b\\right)$ to learn the weights and bias. \n",
    "<br> \n",
    "\n",
    "> Once the weights and bias are learned, we still compute $f\\left(\\mathbf{w}\\cdot\\mathbf{x}+b\\right)$ to classify any particular feature vector $\\mathbf{x}$. The function $f$ from before is now considered a ***quantizer*** (as opposed to the activation function, which is now the identity function) that maps the net value into the class. \n",
    "\n",
    "We summarize this in the figure below. \n",
    "\n",
    "<img src=\"images/perceptron_vs_ADALINE.png\" width=50%>\n",
    "\n",
    "\n",
    "The main difference is thus summarized as follows:\n",
    "> While $y$ is just -1's and 1's, the output of $\\left(\\mathbf{w}\\cdot\\mathbf{x} + b\\right)$ is *continuous*, so the *error* defined by $y^{(i)} - \\left(\\mathbf{w}\\cdot\\mathbf{x}^{(i)} + b\\right)$ does not just take on discrete values but instead provides a number corresponding to *how much* we are right/wrong.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does this change the way in which we learn the weights?\n",
    "\n",
    "We first write the **sum of square errors (SSE)** from the $N$ training data as the *cost function* (sometimes called an *objective* function)\n",
    "$$\n",
    " \\large   J(\\mathbf{w},b) = \\frac{1}{2}\\sum_{i=1}^N \\left[y^{(i)}-\\left(\\mathbf{w}\\cdot\\mathbf{x}^{(i)}+b\\right)\\right]^2. \n",
    "$$\n",
    "\n",
    "Observe that $J(\\mathbf{w},b)$ is a *quadratic* function over the space of weights and bias. The *objective* is to learn the weights and bias that *minimize* the *cost* function. This is conceptualized in the picture below.\n",
    "\n",
    "!<img src=\"images/cost-functional.png\" title=\"The cost function\" width=50%>\n",
    "\n",
    "### There is a unique minimum?\n",
    "\n",
    "***Correct!*** Since the cost function is a quadratic function, it has a unique global minimum even if the data are not linearly separable! (This is the part where you say \"ooohhh\".)\n",
    "\n",
    "This means that you can always train an ADALINE...at least theoretically..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximating the minimum with [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)\n",
    "\n",
    "First, a quick review of some useful facts from calculus and how we use them (you only need to understand how to apply these, so if you are not familiar with calculus, don't worry). \n",
    "\n",
    "- The [gradient](https://en.wikipedia.org/wiki/Gradient) of a scalar valued multivariate function evaluated at some input is a *vector* that points in the direction of greatest increase of function values. This means that the *negative* of the gradient at some input will point in the direction of greatest *decrease* of function values. \n",
    "<br><br> \n",
    "\n",
    "   - Why is this important to us? We want to minimize a function, so we will be using the *negative gradient*.\n",
    "<br><br>\n",
    "\n",
    "   - What do we need to be aware of? The *length* of the gradient vector (also called its magnitude) can complicate the decision making process in terms of how far we should step in the direction of the negative gradient.\n",
    "   <br><br>\n",
    "       - If the length of the gradient vector is too large, then we may step *too far* so that we end up *increasing* the value of the function instead of *decreasing* the value! This is just plain counterproductive. \n",
    "    <br><br>\n",
    "       - If the length of the gradient is too small or we just choose to make too small of steps, then it may take *way too long* to have this method converge. \n",
    "<br><br>\n",
    "\n",
    "The image below may prove useful to refer as it summarizes why we move in the direction of the negative gradient.\n",
    "\n",
    "<img src=\"images/cost-functional-gradient.png\" title=\"The negative direction of gradient is what we want!\" width=\"50%\" />\n",
    "\n",
    "- The gradient is a vector of partial derivatives where the $j$th component of the vector is the partial derivative of the function with respect to the $j$th input variable. \n",
    "<br>\n",
    "\n",
    "   - What does this mean for us? If there are $m$ features in our data, then the gradient is an $(m+1)$-dimensional vector because there are $m$ weights and 1 bias, i.e., $J(\\mathbf{w}, b)$ is a function of $(m+1)$ variables.\n",
    "   \n",
    "     The partial derivatives of $J(\\mathbf{w},b)$ are denoted by\n",
    "$$\n",
    "    \\frac{\\partial J}{\\partial w_j} = -\\sum_{i=1}^N \\Big(\\left[y^{(i)}-\\left(\\mathbf{w}\\cdot\\mathbf{x}^{(i)}+b\\right)\\right]x_j^{(i)}\\Big)\n",
    "$$\n",
    "     and\n",
    "$$\n",
    "    \\frac{\\partial J}{\\partial b} = -\\sum_{i=1}^N \\left[y^{(i)}-\\left(\\mathbf{w}\\cdot\\mathbf{x}^{(i)}+b\\right)\\right].\n",
    "$$\n",
    "     Then, the gradient of $J(\\mathbf{w},b)$, denoted by $\\nabla J(\\mathbf{w},b)$, is given by the $(m+1)$-dimensional vector\n",
    "$$\n",
    "\\large    \\nabla J(\\mathbf{w}, b) = \\left[\\begin{array}{c} \n",
    "                                    \\frac{\\partial J}{\\partial w_1} \\\\\n",
    "                                    \\frac{\\partial J}{\\partial w_2} \\\\\n",
    "                                    \\vdots \\\\\n",
    "                                    \\frac{\\partial J}{\\partial w_m} \\\\\n",
    "                                    \\frac{\\partial J}{\\partial b}\n",
    "                                    \\end{array}\\right].\n",
    "$$\n",
    "\n",
    "Putting the above together, we arrive at the rather simple approach to updating a current guess of weights and bias. \n",
    "Given a ***learning rate***, denoted by $r$, determine the additive updates to the weights and bias using the following\n",
    "$$\n",
    "     \\left[\\begin{array}{c} \n",
    "                 \\Delta w_1 \\\\\n",
    "                 \\Delta w_2 \\\\\n",
    "                 \\vdots \\\\\n",
    "                  \\\\\n",
    "                 \\Delta w_m \\\\\n",
    "                 \\Delta b\n",
    "                 \\end{array}\\right] =  -r\\left[\\begin{array}{c} \n",
    "                                    \\frac{\\partial J}{\\partial w_1} \\\\\n",
    "                                    \\frac{\\partial J}{\\partial w_2} \\\\\n",
    "                                    \\vdots \\\\\n",
    "                                    \\frac{\\partial J}{\\partial w_m} \\\\\n",
    "                                    \\frac{\\partial J}{\\partial b}\n",
    "                                    \\end{array}\\right] = -r\\nabla J(\\mathbf{w}, b)\n",
    "$$\n",
    "Simply put, this means that for $1\\leq j\\leq m$, \n",
    "$$\n",
    "    \\Delta w_j = r\\sum_{i=1}^N \\Big(\\left[y^{(i)}-\\left(\\mathbf{w}\\cdot\\mathbf{x}^{(i)}+b\\right)\\right]x_j^{(i)}\\Big),\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    \\Delta b = r\\sum_{i=1}^N \\left[y^{(i)}-\\left(\\mathbf{w}\\cdot\\mathbf{x}^{(i)}+b\\right)\\right].\n",
    "$$\n",
    "\n",
    "In other words, the learning rate determines how far we move in the direction of the negative gradient (i.e., it dictates the step size). We will see just how important it is to choose this hyperparameter correctly.\n",
    "\n",
    "### What are the epochs now?\n",
    "\n",
    "Note that now we update all of the weights using all of the training data at once. An epoch describes how many times we compute the gradient and update the weights just as before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait one minute, how is this different than finding a linear function of best fit?\n",
    "\n",
    "It is basically the same thing. In fact, the gradient descent used in the ADELINE algorithm to learn the weights and bias will just converge (well, maybe) to what we would get if we just used [least squares](https://en.wikipedia.org/wiki/Linear_least_squares) (which is also related to standard [regression](https://en.wikipedia.org/wiki/Linear_regression) from statistics). \n",
    "\n",
    "Why did I say *maybe* about the convergence? Well, the gradient descent algorithm is iterative and can be a bit sensitive to initial conditions (i.e., initial guesses of weights and bias) and the learning rate $r$ as hinted at above. \n",
    "We explore this in the code cell below.\n",
    "\n",
    "Well gosh, if there are all these issues, then why don't we just use least squares to find the function $\\mathbf{w}\\cdot\\mathbf{x}+b$ instead of gradient descent?\n",
    "Least squares will just give the solution using a closed-form expression. No iterations necessary. \n",
    "\n",
    "This is a good question for you to research. You will find the answer pretty quickly if you read about the [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) method that is actually preferred in practice for big data sets.\n",
    "Towards the end of this lecture, we will start using the stochastic gradient descent.\n",
    "\n",
    "As you learn more about machine learning and computational science in general, it is a good idea for you to conceptualize a taxonomy of scenarios where you may prefer using one method to another. If there are at least two algorithms/approaches to solve any problem, there will always be problems where one method works better than another. It is good to know when and why this happens. This usually requires learning more about mathematics (linear algebra and numerical analysis), statistics (including some basic probability theory), and computer science. You do *not* need to master any or all of these, but it is best to develop a deep understanding in at least one of those areas while being effectively *literate/knowledgeable/fluent* in the language of the others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider the quadratic f(x)=20*x^2 + 2, with derivative 40*x and minimum at x=0\n",
    "\n",
    "def visualize_gradient_descent(x_old, r, num_iter):\n",
    "    f = lambda x: 10*x**2 + 2\n",
    "    df = lambda x: 20*x\n",
    "    \n",
    "    x = np.linspace(-3, 3, 100)\n",
    "    y = f(x)\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    ax.plot(x, y, 'b', linewidth=2, alpha=0.25)\n",
    "    \n",
    "    ax.scatter(x_old, f(x_old), s=50, c='r')\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        x_new = x_old - r*df(x_old)\n",
    "        ax.scatter(x_new, f(x_new), s=50, c='r')\n",
    "        ax.arrow(x=x_old, y=f(x_old), dx=(x_new-x_old), dy=(f(x_new)-f(x_old)))\n",
    "        x_old = x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out\n",
    "\n",
    "interact(visualize_gradient_descent, \n",
    "         x_old = widgets.FloatSlider(value=2, min=0.1, max=3, step=0.1),\n",
    "         r = widgets.FloatText(value=0.025, step=0.01),\n",
    "         num_iter = widgets.IntSlider(value=1, min=1, max=1000, step=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we play!\n",
    "\n",
    "Back to the 1-D and 2-D examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantizer(x, w, b): #Really the same thing as the f_1d or f_2d before\n",
    "    n = x.shape[0]\n",
    "    z = -np.ones(n)\n",
    "    idx = np.where(np.dot(x,w)+b>0)[0]\n",
    "    z[idx] = 1\n",
    "    return z\n",
    "\n",
    "def visualize_updates_1d_ADALINE(x, y, w, b, r, num_epochs):\n",
    "    \n",
    "    x_plot = np.linspace(np.min(x)-0.1*(np.max(x)-np.min(x)), \n",
    "                         np.max(x)+0.1*(np.max(x)-np.min(x)), \n",
    "                         101)\n",
    "    y_plot = quantizer(x_plot,w,b)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    ax.plot(x_plot, y_plot, 'b', linewidth=2, label='initial classification')\n",
    "\n",
    "    ax.set_ylim([-1.25, 1.75])\n",
    "    ax.scatter(x, y, s=50, c='r')\n",
    "\n",
    "    num_training_data = np.size(x)\n",
    "    \n",
    "    for j in range(num_epochs):\n",
    "        errors = y - (np.dot(x,w)+b)\n",
    "        w += r*np.dot(x,errors)\n",
    "        b += r*np.sum(errors)\n",
    "\n",
    "            \n",
    "    y_plot = quantizer(x_plot, w, b)\n",
    "    ax.plot(x_plot, y_plot, 'k:', linewidth=2, label='learned classification')\n",
    "    \n",
    "    ax.plot(x_plot, w*x_plot+b, 'k-.', linewidth=1, label='net input')\n",
    "    \n",
    "    ax.legend(fontsize=20)\n",
    "    \n",
    "    ax.axhline(y=0)\n",
    "    ax.text(np.mean(x_plot), 1.1, r\"Learned $w$=%3.2f and $b$=%3.2f\" %(w,b),\n",
    "            horizontalalignment='center', fontsize=20)\n",
    "    \n",
    "    ax.set_title('$J(w,b)$=%3.2e' %(0.5*np.sum(errors**2)), fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out\n",
    "\n",
    "# Create training data that is linearly separated\n",
    "num_training = 100\n",
    "x_min = -20\n",
    "x_max = 50\n",
    "pt1 = 10 # where to separate into classes\n",
    "pt2 = 15 # remember to make pt2 > pt1\n",
    "# Probability of a training point ending up in [x_min, pt1] vs [x_max, pt2] given by proportion of lengths\n",
    "prob1 = (pt1-x_min)/((pt1-x_min)+(x_max-pt2))\n",
    "prob2 = (x_max-pt2)/((pt1-x_min)+(x_max-pt2))\n",
    "pseudo_data = np.random.uniform(low=0, high=1, size=num_training)\n",
    "x_data = np.zeros(num_training)\n",
    "for i in range(num_training):\n",
    "    if pseudo_data[i]<prob1:\n",
    "        x_data[i] = np.random.uniform(low=x_min, high=pt1)\n",
    "    else:\n",
    "        x_data[i] = np.random.uniform(low=pt2, high=x_max)\n",
    "y_data = separate_line(x_data, pt2)\n",
    "\n",
    "interact(visualize_updates_1d_ADALINE, \n",
    "         x = widgets.fixed(x_data),\n",
    "         y = widgets.fixed(y_data),\n",
    "         w = widgets.FloatSlider(value=0.1, min=-10, max=10, step=0.1),\n",
    "         b = widgets.FloatSlider(value=-1, min=-10, max=10, step=0.1), \n",
    "         r = widgets.fixed(1e-5), \n",
    "         num_epochs = widgets.IntSlider(value=1, min=0, max=1000, step=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now remove the gap in the 1-D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out\n",
    "\n",
    "# Create training data that is separated but without a \"gap\"\n",
    "num_training = 100\n",
    "x_min = -1\n",
    "x_max = 1\n",
    "pt = -0.5# where to separate into classes\n",
    "x_data = np.random.uniform(low=x_min, high=x_max, size=num_training)\n",
    "y_data = separate_line(x_data, pt)\n",
    "\n",
    "interact(visualize_updates_1d_ADALINE, \n",
    "         x = widgets.fixed(x_data),\n",
    "         y = widgets.fixed(y_data),\n",
    "         w = widgets.FloatSlider(value=1, min=-10, max=10, step=0.1),\n",
    "         b = widgets.FloatSlider(value=0.2, min=-10, max=10, step=0.1), \n",
    "         r = widgets.fixed(1e-3), \n",
    "         num_epochs = widgets.IntSlider(value=1, min=0, max=1000, step=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity\n",
    "---\n",
    "\n",
    "1. Create a code cell below to try learning the weights for a perceptron when the data is linearly separable but without a well-defined gap as above. \n",
    "\n",
    "2. Report what you see in a Markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_updates_2d_ADALINE(x, y, w1, w2, b, r, num_epochs):\n",
    "    w = np.array([w1, w2])\n",
    "    \n",
    "    x1_plot = np.linspace(np.min(x[:,0])-0.1*(np.max(x[:,0])-np.min(x[:,0])), \n",
    "                          np.max(x[:,0])+0.1*(np.max(x[:,0])-np.min(x[:,0])), \n",
    "                          51)\n",
    "    x2_plot = np.linspace(np.min(x[:,1])-0.1*(np.max(x[:,1])-np.min(x[:,1])), \n",
    "                          np.max(x[:,1])+0.1*(np.max(x[:,1])-np.min(x[:,1])), \n",
    "                          51)\n",
    "    x1_plot, x2_plot = np.meshgrid(x1_plot, x2_plot)\n",
    "    x_plot = np.vstack((x1_plot.flatten(), x2_plot.flatten())).T\n",
    "    y_plot = f_2d(x_plot, w, b)\n",
    "    y_plot = y_plot.reshape(x1_plot.shape)\n",
    "\n",
    "    fig, axs = plt.subplots(1,2,figsize=(20,10))\n",
    "    \n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    axs[0].contourf(x1_plot, x2_plot, y_plot , alpha=0.2, cmap=cmap)\n",
    "    \n",
    "    idx_1 = np.where(y==1)[0]\n",
    "    idx_0 = np.where(y==-1)[0]\n",
    "    \n",
    "    axs[0].scatter(x[idx_1,0], x[idx_1,1], s=50, c='r', marker='x')\n",
    "    axs[0].scatter(x[idx_0,0], x[idx_0,1], s=50, c='b', marker='o')\n",
    "    axs[0].set_title('Initial perceptron', fontsize=18)\n",
    "    \n",
    "    num_training_data = x.shape[0]\n",
    "    \n",
    "    for j in range(num_epochs):\n",
    "        errors = y - (np.dot(x,w)+b)\n",
    "        w += r*np.dot(x.T,errors)\n",
    "        b += r*np.sum(errors)\n",
    "    \n",
    "    y_plot = f_2d(x_plot, w, b)\n",
    "    y_plot = y_plot.reshape(x1_plot.shape)\n",
    "    \n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    axs[1].contourf(x1_plot, x2_plot, y_plot , alpha=0.2, cmap=cmap)\n",
    "    \n",
    "    axs[1].scatter(x[idx_1,0], x[idx_1,1], s=50, c='r', marker='x')\n",
    "    axs[1].scatter(x[idx_0,0], x[idx_0,1], s=50, c='b', marker='o')\n",
    "    \n",
    "    axs[1].set_title('Learned $w$=(%3.2f,%3.2f) and $b$=%3.2f' %(w[0], w[1], b), fontsize=18)\n",
    "    \n",
    "    axs[1].set_title('$J(w,b)$=%3.2e' %(0.5*np.sum(errors**2)), fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out\n",
    "\n",
    "# Create training data\n",
    "num_training = 100\n",
    "bbox_1 = np.array([[0.5, 0], [1.5, 4]]) #first row is lower-left point and second row is upper-right point of class -1\n",
    "bbox_2 = np.array([[2, 2], [4, 4]]) # \" \" of class 1\n",
    "# Will assume equal probability of training data being in either box\n",
    "pseudo_data = np.random.uniform(low=0, high=1, size=num_training)\n",
    "x_data = np.zeros((num_training,2))\n",
    "y_data = -np.ones(num_training)\n",
    "for i in range(num_training):\n",
    "    if pseudo_data[i]<0.5:\n",
    "        x_data[i,0] = np.random.uniform(low=bbox_1[0,0], high=bbox_1[1,0])\n",
    "        x_data[i,1] = np.random.uniform(low=bbox_1[0,1], high=bbox_1[1,1])\n",
    "    else:\n",
    "        x_data[i,0] = np.random.uniform(low=bbox_2[0,0], high=bbox_2[1,0])\n",
    "        x_data[i,1] = np.random.uniform(low=bbox_2[0,1], high=bbox_2[1,1])\n",
    "        y_data[i] = 1\n",
    "\n",
    "interact(visualize_updates_2d_ADALINE, \n",
    "         x = widgets.fixed(x_data),\n",
    "         y = widgets.fixed(y_data),\n",
    "         w1 = widgets.FloatSlider(value=1, min=-10, max=10, step=0.1),\n",
    "         w2 = widgets.FloatSlider(value=-1, min=-10, max=10, step=0.1),\n",
    "         b = widgets.FloatSlider(value=2, min=-10, max=10, step=0.1), \n",
    "         r = widgets.fixed(1e-3), \n",
    "         num_epochs = widgets.IntSlider(value=1, min=1, max=1000, step=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out\n",
    "\n",
    "interact(visualize_updates_2d_ADALINE, \n",
    "         x = widgets.fixed(bball_features),\n",
    "         y = widgets.fixed(playoffs_All),\n",
    "         w1 = widgets.FloatSlider(value=1, min=-10, max=10, step=0.1),\n",
    "         w2 = widgets.FloatSlider(value=-1, min=-10, max=10, step=0.1),\n",
    "         b = widgets.FloatSlider(value=-2, min=-10, max=10, step=0.1), \n",
    "         r = widgets.fixed(1e-7), \n",
    "         num_epochs = widgets.IntSlider(value=1, min=0, max=1000, step=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional important concepts (i.e., things you need to know for your  assignment)\n",
    "---\n",
    "\n",
    "1. Many machine learning algorithms benefit from ***[feature scaling](https://en.wikipedia.org/wiki/Feature_scaling)*** (sometimes called *data normalization*). Gradient descent in particular benefits from feature scaling. What is feature scaling? It is a *pre-processing* of data. Perhaps the most common feature scaling method is ***standardization***. The basic idea is that by re-scaling the ranges of the individual features into comparable length scales, we can better assess how to weight the individual features based on their importance in explaining the variation in data. \n",
    "\n",
    "> Imagine we collect data on heights and weights of adults, and then try to predict some health outcome based on this data (e.g., whether or not they have arthritis, heart disease, or other conditions that we believe may be linked to an individual's size). If the height is in feet, then the range might be $[4, 7]$, and if the weight is in pounds, then the range might be $[70, 500]$. These disparate magnitudes and ranges of values will often negatively impact our machine learning algorithm. Standardization (also called Z-score normalization) will transform each feature so that its sample mean is zero and sample standard deviation is one. This will generally improve the performance of the machine learning algorithm. \n",
    "\n",
    "You can Google \"importance of standardization of data\" to get more information on the topic. We simply mention it is important here. In your assignment, I include a simple example that illustrates how a standardization of the data can improve gradient descent performance. You will be asked to do some feature scaling of data and compare algorithm performance with and without feature scaling. \n",
    "\n",
    "2. Variants of gradient descent may be required for large data sets. While we are dealing with relatively small data sets for the purposes of learning how to do things, you should be aware of some important variants of algorithms that rely on gradient descent that are useful for \"big data.\" These include [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) and mini-batch gradient descent. \n",
    "\n",
    "These variants will also be explored in your assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since we are talking about classifiers, we might as well talk about [`classes`](https://docs.python.org/3/tutorial/classes.html) and objects (Approx. time: 1 hour)\n",
    "---\n",
    "\n",
    "We are in a good position to now discuss some of the fundamentals of [object oriented programming (OOP)](https://en.wikipedia.org/wiki/Object-oriented_programming). \n",
    "\n",
    "What are objects and their relationship to classes? From the Wiki: \n",
    "\n",
    "> If two objects apple and orange are instantiated from the class Fruit, they are inherently fruits and it is guaranteed that you may handle them in the same way; e.g. a programmer can expect the existence of the same attributes such as color or sugar_content or is_ripe.\n",
    "\n",
    "Abstracting that a bit more, we have that\n",
    "\n",
    "> Objects combine variables and functions into a single entity derived from a class. Classes are essentially a *template* to create your objects.\n",
    "\n",
    "If you approach programming as if you are an [architect](https://www.youtube.com/watch?v=cHZl2naX1Xk), then you can develop a deeper appreciation for OOP and the need for classes.\n",
    "Suppose you want to design a [neural network (NN)](https://en.wikipedia.org/wiki/Neural_network) to perform some complex task like image classification.\n",
    "This NN may involve *many* different \"neuron\" models that are all interconnected to perform the task.\n",
    "You may then decide it is worth constructing a \"neuron\" class from which all the neurons in your NN are just distinct objects (unique instantiations) from this class. \n",
    "Each object holds certain important variables that are unique to that object (e.g., perhaps a string variable stating whether the neuron is modeled as a perceptron or ADALINE along with the weights and bias for the net input function) along with certain functional capabilities (e.g., a \"fit\" function that learns the weights and bias from some training data).\n",
    "\n",
    "Your assignment involves some OOP. Below, we explore the basic concepts you will need for the assignment including an activity you should find useful before starting the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You are already familiar with objects and classes!\n",
    "---\n",
    "\n",
    "Yes, you really are! Numpy [arrays](https://numpy.org/doc/1.18/reference/generated/numpy.array.html) are *objects* from the array *class*. See here for more information: https://scipy-lectures.org/intro/numpy/array_object.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay, how do I create my own classes and objects?\n",
    "---\n",
    "\n",
    "Good question! \n",
    "\n",
    "We will see through the use of a commented example. For more information, check this out: https://docs.python.org/3/tutorial/classes.html or Google some questions!\n",
    "\n",
    "In general, we will make our classes with an instantiation operation (i.e., an initialization function) that is run each type an object of that class type is created.\n",
    "\n",
    "> Many classes like to create objects with instances customized to a specific initial state. Therefore a class may define a special method named `__init__()`, like shown below. When a class defines an `__init__()` method, class instantiation automatically invokes `__init__()` for the newly-created class instance. While not necessary, the `__init()__` method may have arguments for greater flexibility. In that case, arguments given to the class instantiation operator are passed on to `__init__()`. ***We do this below.***\n",
    "\n",
    "***Some notes about conventions:*** \n",
    "\n",
    "- You will see that the \"variable\" `self` is used almost everywhere within classes. This in fact is often the first argument of a method. This is nothing more than a convention: the name self has absolutely no special meaning to Python. But, it allows for more clear readability of code and understanding what in an object is essentially a \"self-referencing\" part of the class.\n",
    "<br><br>\n",
    "\n",
    "- We will initially see that `object` is a variable for a class. This is not necessary in Python 3.+, but is a convention used to distinguish a \"base\" class (also called a \"super\" class) from a \"sub-class\". We will get into sub-classes in a bit because it is relevant for your assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A *silly* example\n",
    "---\n",
    "\n",
    "Below, we create a class called `I_am_what_I_am` that is instantiated with a single input variable that we call `x`. The idea of this class is to turn a variable `x` into a more \"self-aware\" object that can report its type along with other information related to itself. \n",
    "\n",
    "\n",
    "The variable `x` becomes a data *attribute* of an instantiated object and is accessible using the dot convention (we will see examples of this below).\n",
    "\n",
    "The functions defined within the class are called methods and also are *attributes* of an instantiated object that are accessible using the dot convention (again, we will see examples of this below).\n",
    "\n",
    "Recall a numpy array also has some method attributes such as `.min()` and `.max()` that return the minimum and maximum elements of the array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a self-aware object\n",
    "\n",
    "class I_am_what_I_am(object): #a class that turns anything into an object that tells you what type it is\n",
    "    def __init__(self, x): #this initializes the object to know what x is!\n",
    "        self.x = x # the object will keep as an attribute the variable x used to instantiate it\n",
    "    \n",
    "    def what_am_I(self): #print what type of variable x is\n",
    "        print(type(self.x))\n",
    "        \n",
    "    def what_created_me(self): #print x\n",
    "        print(self.x)\n",
    "        \n",
    "    def my_purpose(self): #oh no, what have we created?!\n",
    "        print('That is...classified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "\n",
    "b = I_am_what_I_am(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.what_am_I()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.what_created_me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.my_purpose() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the variable a is now the attribute x\n",
    "b.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if we change the array a?\n",
    "a[2] = 7\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.x #can you explain this?! hint: mutable vs unmutable data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del a #delete a from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.x #can you explain this?! hint: the system has a memory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if I want my object to be more \"stable\"?\n",
    "---\n",
    "\n",
    "Imagine you instantiate an object using mutable data types (lists, numpy arrays, etc.), and you want the object to remain unchanged if these data types change later in your code. (Why might these data types change? You may be creating a sequence of objects within a loop that is iteratively updating an array used in the instantiation of the object.)\n",
    "\n",
    "The [`copy`](https://docs.python.org/3/library/copy.html) function may just be what you want. \n",
    "\n",
    "> Assignment statements in Python do not copy objects, they create bindings between a target and an object. For collections that are mutable or contain mutable items, a copy is sometimes needed so one can change one copy without changing the other. \n",
    "\n",
    "There are \"shallow\" copies and \"deep\" copies that can be created. Read up on this for more information. We will use a deep copy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a different class\n",
    "\n",
    "class I_am_what_I_am_no_matter_what_you_say(object): #a class that turns anything into an object that tells you what type it is\n",
    "    def __init__(self, x): #this initializes the object to know what x is!\n",
    "        self.x = copy.deepcopy(x) # the object will keep as an attribute the variable x used to instantiate it\n",
    "    \n",
    "    def what_am_I(self): #print what type of variable x is\n",
    "        print(type(self.x))\n",
    "        \n",
    "    def what_created_me(self): #print x\n",
    "        print(self.x)\n",
    "        \n",
    "    def my_purpose(self): #oh no, what have we created?!\n",
    "        print('That is...classified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "\n",
    "b = I_am_what_I_am_no_matter_what_you_say(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[2] = 7\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x**2+2\n",
    "\n",
    "c = I_am_what_I_am_no_matter_what_you_say(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.what_am_I()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.what_created_me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.x(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x):\n",
    "    return x**2+2\n",
    "\n",
    "d = I_am_what_I_am_no_matter_what_you_say(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.what_am_I()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.what_created_me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.x(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-classes and super-classes\n",
    "---\n",
    "\n",
    "I like this tutorial for an overview of sub-classes and super-classes that is not overly technical in its presentation: https://realpython.com/python-super/. We will work through and expand upon just part of the example presented there.\n",
    "\n",
    "For a simple use-case example that is kind of silly and easy to read, you may also want to check out this blog: https://pybit.es/python-subclasses.html\n",
    "\n",
    "---\n",
    "\n",
    "Imagine you have a class `polygon` used as a template for all [polygonal](https://en.wikipedia.org/wiki/Polygon) objects. \n",
    "\n",
    "It might look something like what we have below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class polygon(object):\n",
    "    def __init__(self, num_sides):\n",
    "        self.num_sides = num_sides \n",
    "        \n",
    "    def what_am_I(self):\n",
    "        s = 'I am a polygon with ' + str(self.num_sides) + ' sides.'\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine you want to create classes `rectangle` and `square`. \n",
    "\n",
    "Well, these are just special types of 4-sided polygons. Moreover, a square is just a special type of rectangle! \n",
    "\n",
    "In that case, you can create `rectangle` as a sub-class of `polygon`, and you can create `square` as a sub-class of `rectangle`. See below and *pay attention to the arguments in these class declarations*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rectangle(polygon): #rectangle is a sub-class of polygon \n",
    "    def __init__(self, length, width):\n",
    "        super().__init__(num_sides=4) #Now rectangle inherits from polygon\n",
    "        self.length = length\n",
    "        self.width = width\n",
    "        \n",
    "    def compute_area(self):\n",
    "        self.area = self.length * self.width\n",
    "    \n",
    "    def my_area(self):\n",
    "        try:\n",
    "            print(self.area)\n",
    "        except AttributeError:\n",
    "            self.compute_area()\n",
    "            print(self.area)\n",
    "            \n",
    "    def compute_perimeter(self):\n",
    "        self.perimeter = 2*self.length + 2*self.width\n",
    "        \n",
    "    def my_perimeter(self):\n",
    "        try:\n",
    "            print(self.perimeter)\n",
    "        except AttributeError:\n",
    "            self.compute_perimeter()\n",
    "            print(self.perimeter)\n",
    "            \n",
    "    def my_dimensions(self):\n",
    "        s = 'Length = ' + str(self.length) + '\\n' + 'Width = ' + str(self.width)\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = rectangle(length=2, width=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.what_am_I()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.my_area()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.my_perimeter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.my_dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class square(rectangle): #A square inherits from rectangle\n",
    "    def __init__(self, length): \n",
    "        super().__init__(length=length, width=length)\n",
    "    \n",
    "    def my_dimensions(self): #This over-rides the inherited method from rectangle\n",
    "        s = 'Length = Width = ' + str(self.length)\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = square(length=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B.my_area()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B.my_dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B.what_am_I()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Venturing into the third-dimension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cube(object):\n",
    "    def __init__(self, edge_length):\n",
    "        self.length = edge_length\n",
    "        self.faces = square(length=edge_length)  #faces are squares!\n",
    "        self.faces.compute_area()\n",
    "        self.compute_surface_area()\n",
    "        self.compute_volume()\n",
    "    \n",
    "    def compute_surface_area(self):\n",
    "        self.surf_area = self.faces.area * 6\n",
    "\n",
    "    def compute_volume(self):\n",
    "        self.volume = self.faces.area * self.length\n",
    "    \n",
    "    def what_am_I(self):\n",
    "        s = 'I am a cube with edge length = ' + str(self.length)\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = cube(edge_length=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C.surf_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C.what_am_I()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C.faces.what_am_I()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C.volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity\n",
    "---\n",
    "\n",
    "1. Use some code-cells below to construct a `neuron` class along with `perceptron` and `ADALINE` sub-classes. Refer to code above and the source material (https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch02/ch02.ipynb) for help. Figure out what common variables/functions should be attributes within the `neuron` class (e.g., the net input function and prediction functions are the *same* for each neuron) and what should be unique to the sub-classes (e.g., the fit/learning of weights and bias). \n",
    "\n",
    "2. Instantiate some perceptron and ADALINE objects and apply them to data sets seen in this notebook. Provide visualizations and analysis of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
